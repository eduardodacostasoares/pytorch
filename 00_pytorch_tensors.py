# -*- coding: utf-8 -*-
"""00_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105zOlIshMylQMM8DT50r82uaKqPIOdPZ

## 00. PyTorch Fundamentals
Link: https://www.learnpytorch.io/00_pytorch_fundamentals/
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print(torch.__version__)

"""## 1. Tensors

### Creating tensors

PyTorch tensors are created by the command 'torch.tensor()' = https://pytorch.org/docs/stable/tensors.html
"""

# Scalar: is a tensor made by 1 element

scalar = torch.tensor(7)
scalar

scalar.ndim

#Get tensor back as a Python integer
scalar.item()

# Vector: is a tensor made by more than 1 element

vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

# MATRIX

MATRIX = torch.tensor([[9,2,1],
                       [6,4,3],
                       [5,7,8]])
MATRIX

MATRIX.ndim

MATRIX.shape

TENSOR = torch.tensor([[[1],
                        [4]],

                       [[15],
                        [22]],

                       [[49],
                        [52]]])

TENSOR

TENSOR2 = torch.tensor(np.ones((3,3,3)))

TENSOR2

TENSOR.shape

TENSOR.ndim

"""### Random tensors

Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.

`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers`

Torch random tensor = https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand
"""

# Create a random tensor of size (3,4)
random_tensor = torch.rand(3,4)
random_tensor

random_tensor.ndim

# Create a random tensor with a similar shape to an image tensor
random_image_size_tensor = torch.rand(size=(3,224,224)) #colour channels (Red, Green, Blue), height and width.
random_image_size_tensor.shape, random_image_size_tensor.ndim

R_TENSOR = torch.rand(4,2,3)
R_TENSOR.shape, R_TENSOR.ndim

R_TENSOR

"""### Zeros and Ones"""

# Create a tensor of all zeros

zero = torch.zeros(size=(3,4))
zero

# Create a tensor of all ones

ones = torch.ones(size=(3,4))
ones

ones.dtype

# Creating a range of tensors and tensors-like

# Use torch.range(0,10) - 0 where start, 10 where stop (NOT INCLUDING 10)
torch.arange(start=0,end=10, step=2)

#Creating tensors-like
ten_zeroes = torch.zeros_like(input=ones)
ten_zeroes

negative = torch.arange(start=105, end=-1 , step=-3)
negative

"""## Tensor datatypes

**Note:** Tensor datatypes is one of the 3 big errors you'll run into with Pytorch and Deep learning:

1. *Tensors not right datatype*;
2. *Tensors not right shape*;
3. *Tensors not on the right device*.

Precision in computing: https://en.wikipedia.org/wiki/Precision_(computer_science)

"""

#Float 32 tensor (By default, any data type in PyTorch is "float 32")
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None,          # what datatype is the tensor (e.g. float32, float16 or int)
                               device='cpu',         # what device is your tensor on (e.g. CPU, MPS or CUDA)
                               requires_grad=False) # whether or not to track gradients with this tensors operations

float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

int_tensor = torch.tensor([1,2,3],
                          dtype=int,
                          device='cpu',
                          requires_grad=False)

float_16_tensor*int_tensor

int_32_tensor = torch.tensor([3,6,9], dtype=torch.long)
int_32_tensor

float_32_tensor*int_32_tensor



"""### Getting information from tensors (tensor attributes

1. *Tensors not right datatype*; - to get datatype from a tensor, can use `tensor.dtype`
2. *Tensors not right shape*; - to get shape from a tensor, can use `tensor.shape`
3. *Tensors not on the right device*. to get device from a tensor, can use `tensor.device`  
"""

# P.S.: torch.rand(x,y) returns numbers between 0 and 1, in a specific size (x rows and y columns)
# P.S.2: torch.randn(x,y) returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1

tensor1 = torch.rand(size=(3,4),
                     dtype=torch.float16,
                     device=None,
                     requires_grad=True)
tensor1

print(tensor1)
print(f"Datatype of tensor1: {tensor1.dtype}")
print(f"Shape of tensor1: {tensor1.shape}")
print(f"Device tensor is on: {tensor1.device}")

tensor2 = torch.ones((3,3,3),
                     dtype=torch.int,
                     device='cpu',
                     requires_grad=False)

print(tensor2)
print(f"Datatype of tensor2: {tensor2.dtype}")
print(f"Shape of tensor2: {tensor2.shape}")
print(f"Device tensor is on: {tensor2.device}")

"""### Manipulating tensors (tensors operations)

Tensor operations include:
* Addition
* Subtraction
* Multiplication (element-wise) or Scalar Multiplication
* Division
* Matrix multiplication
"""

# Create a tensor and add 10 to it

tensor = torch.tensor([1,2,3])
tensor + 10

# Multiply tensor by 10

tensor * 10

# Subtract to 10
tensor - 10

# try out Pytorch in-built functions (P.S.: priorize to use Python operations: (tensor + x), (tensor - x), (tensor * x) or (tensor / x)
# Multiply: torch.mul(tensor, y)

torch.mul(tensor,10)

# Addiction: torch.add(tensor, x)

torch.add(tensor, 10)

# Subtraction: torch.sub(tensor, x)
torch.sub(tensor, 10)

"""### Matrix Multiplication

Two main ways of perfortming multiplication in neural networks and deep learning:

1. Element-wise multiplication (or *Scalar Multiplication*);
2. Matrix Multiplication or *dot product* (most common tensor operation).

Matrix Multiplication: https://www.mathsisfun.com/algebra/matrix-multiplying.html

There are two main rules that performing matrix multiplication needs to satisfy:

1. The **inner dimensions** must match:
* Matrix A (n x b) can be multiplied with other Matrix B (b x a)
E.g. `(3, 2) @ (2, 3)`

* `(3, 2) @ (3, 2)` = won't work


2. The resulting MATRIX has the shape of **outer dimensions**
* `(3, 2) @ (2, 3)` = will work (result: MATRIX`(3, 3)`)
* `(3, 4) @ (4, 10)` = will work (result: MATRIX `(3, 10)`)

`@` is the simbol to Matrix Multiplication.
"""

# Element wise-multiplication - tensor = [1, 2, 3]

tensor * tensor

# Matrix Multiplication - torch.matmul(tensor, tensor)
torch.matmul(tensor, tensor)

# Matrix Multiplication by hand
tensor.shape

# Commented out IPython magic to ensure Python compatibility.
# # Comparing time between "matmul()" and by hand
# %%time
# value = 0
# for i in range(len(tensor)):
#   value += tensor[i] *tensor[i]
# print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor, tensor)

"""### One of the most common errors in deep learning: shape errors



"""

# Matrix (q x n) can only be multiplied by other Matrix (n x p)
torch.matmul(torch.rand(3,2), torch.rand(3,2))

# Using the previous example, Matrix (3x2) can only be multiplied by another MATRIX (2 x n)
# n > 0

torch.matmul(torch.rand(3,2), torch.rand(2,4))

# shapes for matrix multiplication
tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])

tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

torch.matmul(tensor_A, tensor_B) #torch.mm is the same as torch.matmul (it's an alias for writing less code)

"""To fix our tenshor shape issues, we can manipulate the shape of one of our tensors using a **transpose**

A **transpose** switches the axes or dimensions of a given tensor.
"""

tensor_B.T, tensor_B.T.shape

tensor_B, tensor_B.shape

#The matrix multiplication operation works when tensor_B is transposed
print(f"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}")
print(f"New shapes: tensor_A = {tensor_A.shape} (same shape as above), tensor_B.T = {tensor_B.T.shape}")
print(f"Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions mus match")
print("Output: \n")
output = torch.matmul(tensor_A, tensor_B.T)
print(output)
print(f"\nOutput shape: {output.shape}")

"""## Finding the min, max, mean, sum, etc (tensor aggragation)"""

x = torch.arange(0,100, 10)
x, x.dtype

# Min
torch.min(x), x.min()

# Max
torch.max(x), x.max()

#mean
torch.mean(x) #x is a int dtype, mean() requires float or complex

torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

# Find the sum
torch.sum(x), x.sum()

"""## Finding the positional min and max"""

x

# FInding the position of the minimum value (return the index)
x.argmin()

# Finding the position of the maximum value
x.argmax(), x[9]

"""## Reshapoing, stacking, squeezing and unsqueezing tensors

* Reshaping - reshapes an input tensor to a defined shape
* View - return a view of an input tensor of certains shape but keep the same memory
* Stacking - combine multiples tensors on top of each other.
* Squeeze - remove all `1` dimensions from a tensor;
* Unsqueeze - add a `1` dimension to a target tensor;
* Permute Return a view of the input with dimensions permuted (swapped) in a certain way.
"""

x = torch.arange(1., 10.)
x, x.shape

# add an extra dimension
x_reshaped = x.reshape(9, 1)
x_reshaped, x_reshaped.shape

# Change the view
z = x.view(1,9)
z, z.shape

# Changing z changes x (because a view of a tensor shares the same memory location)
z[:,0] = 11

z, x

# Stack tensors on top of each other
x_stacked = torch.stack([x,x,x,x], dim=1)
x_stacked2 = torch.stack([x,x,x,x], dim=0)

x_stacked, x_stacked2

# torch.squeeze() - remove all single dimensions from a target tensor
print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape: {x_reshaped.shape}")

#remove extra dimensions from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}")
print(f"New Shape: {x_squeezed.shape}")

# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dimension
print(f"Previous target: {x_squeezed}")
print(f"Previous shape: {x_squeezed.shape}")

#Add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=1)
print(f"\nNew tensor: {x_unsqueezed}")
print(f"New shape: {x_unsqueezed.shape}")

# torch.permute() - rearranges the dimensions of a target tensor in a specified order
x_original = torch.rand(size=(224,224,3)) #(height, width, colour_channels)
x_original[:,:,0] = 8
# Permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2, 0, 1)


print(f"Previous shape: {x_original.shape}")
print(f"New shape: {x_permuted.shape}") # New order: (colour_channel, height, width )

"""## Indexing (selecting data from tensors)

Indexing with PyTorch is similar to indexing with NumPy
"""

#Create a tensor
x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

# Indexing on our new tensor outer bracket (zeroth dimension)
x[0]

# Indexing on the middle bracket (dim=1)
x[0][0]

# Indexing on the most inner bracket (last dimension)
x[0][1][1]

x[0][2][2]

# Get all values of 0th and 1st dimension, but only index 1 of the second dimension
x[:,:,1]

# Get all valus of 0 dimension but only the 1 index value of 1st and 2nd dimension
x[:,1,1] #different from x[0][1][1], that select 1 element, x[:,1,1] selects all dimension, not only one value

# Get index 0 of 0th and 1st dimension and all values of 2nd dimension

x[0,0,:]

#index to get 9 and [3,6,9]
x[0,2,2], x[0,:,2]

"""## Pytorch tensors and NumPy

NumPy is a popular scientific numerical computing library.

And because of this, PyTorch has functionality to interact with it.

* Data in NumPy, want in PyTorch tensor -> `torch.fromnumpy(ndarray)`
* PyTorch tensor -> NumPy -> `torch.Tensor.numpy()`

- When creates a tensor from NumPy Array, the type is changed from `float32` (default in PyTorch) to `float64` (the latter, default in NumPy).
"""

# NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array) #warning: the tensor dtype will
array, tensor

array.dtype

tensor.dtype

torch.arange(1.0,8.0).dtype

# Changing the type from "float64" to "float32" of the TENSOR

array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array).type(torch.float32)
array.dtype, tensor.dtype

array1 = np.arange(1.0,8.0)
tensor1 = torch.from_numpy(array1)
array1, tensor1

# Changing the value of array

array1 = array1 + 1
array1, tensor1

# Tensor to NumPy array

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()

tensor, numpy_tensor

#Changing the tensor (What happens to "numpy array"?)

tensor = tensor + 1
tensor, numpy_tensor

"""## Reproducibility (trying to take the random out of random)

In short how a neural network learns:

` Start with random numbers(1) -> tensor operations (2) - > update random numbers to try and make them better representations of the data (3) -> repeat step (2) -> repeat step (3)`

To reduce the randomness in neural networks and PyTorch comes the concept of **random seed**
* Randomness: https://pytorch.org/docs/stable/notes/randomness.html

Essentially what the random seed does is "flavour" the randomness.
* Random seed: https://en.wikipedia.org/wiki/Random_seed
"""

random_tensor_A = torch.rand(3, 4)

random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

# Random but reproducible tensors
# Set the random seed

RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3,3)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3,3)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)

"""## Running tensors and pytorch objects on GPUs (and making faster computations)

GPUs = faster computation on numbers, thanks CUDA + Nvidia hardware + Pytorch working behind the scenes

### 1. Getting a GPU

1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)
2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU.
3. Use cloud computing - GCP, AWS, Azure (these services allow you to rent computers on the cloud and access them).

For 2 and 3 PyTorch + GPU drivers (CUDA)
takes a little bit of setting up to do this, refer to PyTorch setup documentation: https://pytorch.org/get-started/locally/
"""

!nvidia-smi

"""### 2. Check for GPU access with PyTorch

* https://pytorch.org/docs/stable/notes/cuda.html#best-practices
"""

# Check for GPU access with PyTorch
import torch
torch.cuda.is_available()

"""For PyTorch, since it's capable of running compute on the CPU or GPU, it's best pratice to setup device agnostic code:

E.g. run on GPU if available, else default to CPU
"""

# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# count number of devices

torch.cuda.device_count()

"""## 3. Putting tensors (and models) on the GPU

The reason we want our tensors/models on the GPU is because using GPU results in faster computations.
"""

# by default, the tensor is created on CPU (it can be changed using the attibute "device=")
tensor = torch.tensor([1, 2, 3])

print(tensor, tensor.device)

# move tensor to GPU, if available

tensor_on_gpu = tensor.to(device)
tensor_on_gpu

"""### 4. Moving tensors back to CPU"""

# If tensor is on GPU, can't transform in to NumPy
tensor_on_gpu.numpy()

# To fix the GPU tensor with NumPy issue, we can first set it to the CPU

tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu

# The function .cpu() only create a copy of the tensor to the CPU

tensor_on_gpu

"""## Exercises & Extra-curricular"""

# 2) Create a random tensor with a shape (7,7)

tensor7 = torch.rand(7,7)
print(tensor7, tensor7.shape)

# 3) Perform a matrix multiplication on the tensor from 2 with another random tensor with shape (1, 7) (hint: you may have to transpose the second tensor).

tensor8 = torch.rand(1,7)
tensor_product = torch.matmul(tensor7, tensor8.T)
tensor_product

# 4) Set the random seed to 0 and do exercises 2 & 3 over again.
RANDOM_SEED = 0

torch.manual_seed(RANDOM_SEED)
tensor7 = torch.rand(7,7)

torch.manual_seed(RANDOM_SEED)
tensor8 =torch.rand(1,7)

print(torch.matmul(tensor7, tensor8.T))

# 5) Speaking of random seeds, we saw how to set it with torch.manual_seed()
# but is there a GPU equivalent? (hint: you'll need to look into the documentation
# for torch.cuda for this one). If there is, set the GPU random seed to 1234.
RANDOM_SEED = 1234
torch.cuda.manual_seed(RANDOM_SEED)

# 6) Create two random tensors of shape (2, 3) and send them both to the GPU
# (you'll need access to a GPU for this). Set torch.manual_seed(1234) when
# creating the tensors (this doesn't have to be the GPU random seed).

device = "cuda" if torch.cuda.is_available() else "cpu"

torch.manual_seed(RANDOM_SEED)
tensor = torch.rand(2,3)
tensor.to(device)

torch.manual_seed(RANDOM_SEED)
tensor2 = torch.rand(2,3)
tensor2.to(device)

# 7) Perform a matrix multiplication on the tensors you created in 6
# (again, you may have to adjust the shapes of one of the tensors).

tensor_product = torch.matmul(tensor.T, tensor2)
print(tensor_product)

# 8) Find the maximun and minimun values of the output of the 7
print(tensor_product, tensor_product.max(), tensor_product.min())

# 9) Find the maximum and minimum index values of the output of 7.
print(tensor_product.shape, tensor_product.argmax(), tensor_product.argmin())

# 10) Make a random tensor with shape (1, 1, 1, 10) and then create a new tensor
# with all the 1 dimensions removed to be left with a tensor of shape (10).
# Set the seed to 7 when you create it and print out the first tensor
# and it's shape as well as the second tensor and it's shape.

torch.manual_seed(7)
tensor = torch.rand(1,1,1,10)
tensor2 = torch.squeeze(tensor)

print(f"First Tensor:  {tensor}")
print(f"\nShape of first tensor:  {tensor.shape}")
print(f"\n\nSecond Tensor: {tensor2}")
print(f"\nShape of second tensor: {tensor2.shape}")

